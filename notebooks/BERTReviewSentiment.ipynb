{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Review Sentiment\n",
        "\n",
        "Testing out BERT by predicting the sentiment of ten thousand randomly sampled English-language Steam reviews.\n",
        "\n",
        "First, we will need to install any necessary Python packages."
      ],
      "metadata": {
        "id": "9bkKxbueRhqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "j4hhJe2XfkP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then import and set up the required modules and initialise some constant variables."
      ],
      "metadata": {
        "id": "KFpd0QVEfl3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from csv import reader as csv_reader\n",
        "from google.colab import drive\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# constants\n",
        "PATH_GDRIVE = '/content/drive'\n",
        "PATH_DATA = 'drive/MyDrive/MSc Dissertation/review_sentiments/dataset_50k.csv'\n",
        "PATH_MODEL = 'drive/MyDrive/MSc Dissertation/review_sentiments/model_50k'\n",
        "KEY_TEXT = 'text'\n",
        "KEY_LABEL = 'polarity'\n",
        "BERT_MODEL = 'bert-base-uncased'\n",
        "\n",
        "# module initialisation\n",
        "drive.mount(PATH_GDRIVE)\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "LuQft7QqRTrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then create functions to load, split and tokenize the review data. Our dataset will be split into three sets: a training set and a validation set for training a predictive model and a test set for testing our trained model."
      ],
      "metadata": {
        "id": "snEIS3grYMNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    data = { KEY_TEXT: [], KEY_LABEL: [] }\n",
        "    with open(PATH_DATA, 'r', encoding='utf-8') as f:\n",
        "        reader = csv_reader(f, delimiter=',')\n",
        "        for review in reader:\n",
        "            data[KEY_TEXT].append(review[0]) # review text (string)\n",
        "            data[KEY_LABEL].append(int(review[1])) # review polarity (int: 0, 1)\n",
        "    return pd.DataFrame.from_dict(data).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def split_data(data):\n",
        "    model_data, test_data = train_test_split(data, test_size=0.5)\n",
        "    train_data, val_data = train_test_split(model_data, test_size=0.2)\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "def train_model(data, tokenizer, model):\n",
        "    return\n",
        "\n",
        "def split_and_tokenize_data(data):\n",
        "    # split data\n",
        "    train_x, val_x, train_y, val_y = train_test_split(\n",
        "        data[KEY_TEXT].tolist(),\n",
        "        data[KEY_LABEL].tolist(),\n",
        "        test_size=0.2\n",
        "    )\n",
        "    # load tokenizer and create some helper lambdas\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "    encode = lambda x: tokenizer(x, truncation=True, padding=True)\n",
        "    format = lambda x, y: tf.data.Dataset.from_tensor_slices((dict(x), y))\n",
        "    # encode and format the data\n",
        "    train_data = format(encode(train_x), train_y)\n",
        "    val_data = format(encode(val_x), val_y)\n",
        "    return train_data, val_data\n",
        "    #model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=2)\n",
        "\n",
        "def main():\n",
        "    # load and split the data\n",
        "    data = load_data()\n",
        "    train_data, val_data, test_data = split_data(data)\n",
        "    # load a pre-trained tokenizer and classifier\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "    classifier = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=2)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "Z7JcxdqmYKQf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "c033a5045b155981ee72213b5a792ae49fdd09af88109a854dac011f87e13240"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "BERTReviewSentiment.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}