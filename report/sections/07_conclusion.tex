\chapter{Conclusion} \label{sec:Conc}

In this report, state-of-the-art text representation models were trained to predict a selection of Steam review features using only the written text of the review. In the case of review polarities, the results showed that the trained models could predict this feature with a very high degree of accuracy. In additional experiments, certain factors, such as the length of the reviews and the languages they were written in, were controlled for, and the results were evaluated and compared. In each case, the trained models produced consistently strong results.

The results of the models trained to predict the number of votes that reviews received indicated that the employed approach did not produce predictions that were accurate. The models trained to predict how long each reviewer had spent playing the game they were reviewing did appear to be capable of making predictions that were partially accurate, at least when compared to a suitable baseline.

Influenced by the success of the models trained to predict review polarity, the same state-of-the-art text representation techniques were used to predict the average ratings of the games being reviewed using each individual sample of review text. Certain trained models, despite the abundance of bad or noisy data, were able to make predictions that were more accurate than their respective baselines to a significant enough degree that further investigation appeared to be warranted. These trained models were then used to determine which users could be considered the most representative of the total population based on their individual, average accuracies, relative to a baseline predictor. The apparent feasibility of this approach suggests that particularly representative users could be determined using only the raw review text, without any manual selection, filtering or preparation processes being applied to the training data.

The research objectives outlined in this report's introduction were carried out successfully, with intriguing results being provided in every case. The investigation into representative users appears to be the most compelling, for two reasons in particular. Firstly, there are many refinements and improvements that could be made to both the methodology underpinning the approach and the implementation of the technical solution. Secondly, the determination of representative users might potentially have real-world, commercial applications. For example, the approach could be used by media recommendation systems to estimate the quality of newly released products by initially recommending the product to particularly representative users and accurately extrapolating, based on their responses, how popular the product will be among the overall user base. This estimation of quality and potential popularity could then be incorporated into the recommendation system's behaviour to help it decide how frequently or aggressively it should suggest the product in question to average users.
 
\section{Future Work} \label{sec:Conc_FW}

Many of the alternatives or improvements to the approach taken when predicting review polarity have been discussed already, mainly involving the training of the BERT models on larger samples of data. Language-specific samples could also be created for some of the more prevalent languages in the dataset, such as Russian or Chinese, and these could be used to fine-tune BERT models which have already been trained for these particular languages. Additional features could be controlled for when selecting review texts to sample, such as the number of votes the review received or the playtime of the reviewer. The results from these more restrictive samples could provide insight into the factors which affect how accurately review polarity can be predicted. An interesting, though slightly tangential approach, would be to apply a simplified version of the `representative user' analysis method to this task and determine if there exist users whose reviews are particularly easy to classify, polarity-wise.

Much of the additional work that could be done regarding the prediction of review votes and review playtime has already been discussed. The suggested alternative approaches mainly involved transforming the problem from one of regression into one of classification. This would essentially involve the models predicting percentile-based classes rather than exact values, with regard to either the number of votes reviews received or the amount of playtime reviewers had logged. Data balancing techniques, such as excluding reviews without any votes, were also considered and discussed.

The amount of training data supplied to the models used to identify representative users could be hugely increased. In order to increase the quality of the training data, more restrictive sampling methods could be employed, such as only selecting reviews with a certain number of votes or containing a certain number of words. More balanced data samples could also be created in order to reduce the heavy bias towards games with a rating of `positive'.

One of the benefits of the implementation outlined in this report is that it does not require any manual intervention with regard to the labelling of review classes or the filtering of data; however, an obvious, drawback to this approach is the inclusion of bad data in the training set. A number of fully automated or partially automated processes could be used to reduce the prevalence of poor quality or unrepresentative reviews among the training data.

For example, reviews whose texts are not accurately classified by the polarity prediction models could be excluded. Certain text features, such as the readability or writing level, could be considered when sampling the data, while more complex methods, such as those discussed in sections ? and ?, could also be incorporated into the sampling process. Finally, suitable training data could be manually selected and, although it would be time-consuming, this approach would almost certainly lead to a far more accurate model. In summary, though only a small set of potential improvements to the approach utilised in this report have been listed above, it seems likely that this technique could be refined and developed to be considerably more successful.

The model used to determine representative users could be expanded to consider the communities or `clusters' that each user is a part of, thus allowing the model to determine not only how representative individual users are of the entire population, but also of certain subsets of the population.

The dataset also consists of millions of data points concerning the social relationships between reviewers, ie friendship and group membership. These portions of the dataset, though briefly discussed and examined in section ?, were not actually utilised in the design, implementation or training of any of the models used throughout this report. In future work, these data could be used to, for instance, determine potentially influential users, ie those whose reviews result in their friends or fellow group members being more likely to play the games being reviewed. The knowledge of which users are influential and which users are representative could be combined and used to improve the way in which games are recommended by Steam. For example, after initially recommending a new game to a pool of representative users, the recommendation system could then, if it has estimated that the game is of high quality, recommend it to particularly influential users in order to help spread it a wider audience.
