\chapter{Literature Review}  \label{sec:LR}

\section{Natural Language Processing} \label{sec:LR_NLP}

In computer science and, in particular, artificial intelligence, the field of natural language processing (NLP) concerns the representation, analysis and utilisation of human language, in both written and spoken form, by computers. NLP traces its origins to the 1950s when an IBM computer was used to translate a number of Russian sentences into English at Georgetown University \cite{Hutchins2004_MT}.

Modern applications of NLP are ubiquitous, ranging from spam detection \cite{Jindal2007_Spam} to chatbots \cite{Adamopoulou2020_Chatbot}. Specific examples of its use include machine translation tools, such as Google Translate \cite{GoogleTranslate}, that can translate text from one language to another; speech recognition software, such as Microsoft's Cortana \cite{MicrosoftSpeechRecognition}, which can recognise spoken words from audio of human speech; sentiment analysis techniques to determine the sentiment or the emotions conveyed through text which have been used to determine public opinion on topics such as political preferences \cite{Ceron2014_Sentiment}; and question answering systems, such as IBM's Watson, that can respond to questions asked by humans and have been used to make decisions regarding the treatment of cancer patients \cite{AOCNP2015_Watson}.

The state-of-the-art in human language encoding and representation involves the use of transformers, a type of neural network developed by Google \cite{Vaswani2017_Transformers}, which differ from recurrent neural networks (RNN), the prior state-of-the-art, in that they attempt to simulate the process of attention in humans by increasing the weight of certain segments of input data and decreasing the weight of others \cite{Bahdanau2014_Attention}.

Bidirectional Encoder Representations from Transformers (BERT) is an NLP model developed by Google which, when published in 2018, achieved state-of-the-art results in numerous popular NLP tasks such as GLUE and SQuAD v1.1 and v2.0 \cite{Devlin2018_BERT}. BERT, unlike some other language representation models, such as word2vec \cite{Mikolov2013_W2V}, considers the context of each token in any given text data, ie the tokens that come before and after it. An advantage of BERT is, according to \cite{Devlin2018_BERT}, that it ``can be fine-tuned with just one additional output layer \ldots without substantial task-specific architecture modifications''. BERT is used to process almost all English-language Google Search queries \cite{GoogleSearchBERT}.

\subsection{Product Review Sentiment} \label{sec:LR_NLP_Sent}

Substantial research has been done in the field of NLP with regards to text classification and, in particular, sentiment analysis \cite{mironczuk2018recent}. Much of this research has concerned the classification of user sentiment in product reviews.

Support vector machine (SVM) and naive Bayes (NB) classifiers were use to predict the polarities of Amazon book reviews, represented using the term frequency-inverse document frequency (TF-IDF) model, in \cite{dey2020comparative}. The SVM classifier, which had an accuracy score of 84\%, slightly outperformed the NB classifier, which had an accuracy score of 82.8\%. In \cite{aljuhani2019comparison}, the polarities of a balanced set of Amazon phone reviews were predicted using, among others, NB, stochastic gradient descent (SGD) and convolutional neural network (CNN) classifiers. TF-IDF and neural network techniques, such as word2vec, were used to represent the review text. The combination of the CNN classifier and word2vec produced the most accurate results, 79.6\%, while the combination of the NB classifier and TF-IDF had an accuracy of 74.9\%. The SGD classifier performed slightly better than the NB classifier. In \cite{tripathy2016classification}, the polarities of IMDb movie reviews were predicted using NB, SGD and SVM classifiers combined with TF-IDF. The SVM, NB and SGD classifiers had accuracy scores of 88.9\%, 86.2\% and 85.1\%, respectively.

BERT has also been used to predict the polarities of product reviews. In \cite{geetha2021improving}, the polarities of Amazon reviews were predicted using a fine-tuned BERT classifier. For comparison, combinations of NB and SVM classifiers and TF-IDF were also used to make predictions. The BERT classifier, with an accuracy of 88.4\%, outperformed the NB and SVM classifier which had accuracies of 80.1\% and 81.3\%, respectively. An RNN classifier was also used for comparison and produced an accuracy of 83.9\%. A selection of BERT classifiers were used to predict the polarities of a sample of movie reviews from the Czech-Slovak movie database in \cite{lehevcka2020bert}. NB and SVM classifiers, combined with TF-IDF, were again used as a comparison. The BERT classifiers resulted in an average accuracy of 92.7\% while the NB and SVM classifiers resulted in accuracies of 90.6\% and 90.5\%, respectively. Finally, in \cite{gonzalez2020comparing}, the performance of BERT in the prediction of movie review polarities was compared to SVM, NB and ridge classifiers. The BERT classifier had an accuracy of 93.8\% while the SVM, NB and ridge classifiers had accuracies of 89.9\%, 87.7\% and 89.9\%, respectively.

Concerning the Steam platform in particular, it does not appear that any academic research has been done on using BERT to predict the polarity of game reviews. However, other classifiers, such as NB and SVM classifiers, have been trained to make such predictions. A dataset containing over 7 million Steam reviews, of which 83\% were positive, was gathered in \cite{zuo2018sentiment} and a balanced sample of those reviews were used to train NB and decision tree classifiers. The decision tree classifier, which outperformed the NB classifier, had an accuracy of 74.9\%. In \cite{sobkowicz2016steam}, a dataset containing over 3 million reviews, of which 81\% were positive, was gathered and used to train NB and maximum entropy classifiers. The NB classifier produced an f1-score of 0.75 while the maximum entropy classifier produced an f1-score of 0.9. A dataset consisting of over 400 thousand reviews, of which 70\% were positive, was used in \cite{miao2021compare} to train NB, SVM and random forest classifiers which produced f1-scores of 0.88, 0.87 and 0.89, respectively.

\subsection{Product Review Helpfulness} \label{sec:LR_NLP_Help}

Much of the research concerning the prediction of the `helpfulness' of product reviews involves the training of machine learning models using semantic features of the review text, such as its readability or its word count; the history and behaviour of the reviewer; or the pricing and popularity of the product itself \cite{yang2015semantic} \cite{lee2014predicting} \cite{zhang2018predicting}.

In \cite{xu2020bert}, BERT was used to predict the helpfulness of 10 thousand Amazon reviews using, among other input features, the review text. A review's helpfulness was determined by the number of `helpful' votes it had received. The model was evaluated using the mean average error metric and the results were described by the authors as being ``reliable in predicting the helpfulness scores of customer reviews''. BERT was also used in \cite{alsmadi2020predicting} to predict the helpfulness of Amazon reviews in a dataset containing over 83 million reviews. A review was labelled as helpful or unhelpful based on the number of `helpful' votes it received relative to other reviews of the product in question. For comparison, an SVM classifier was also trained. The SVM classifier had an accuracy of 75.9\% while the BERT classifier had an accuracy of 86.1\%. In \cite{wang2020study}, BERT was used to predict the helpfulness of a sample of Amazon reviews containing an equal number of helpful and unhelpful reviews. A review was labelled as helpful or unhelpful based on the ratio of `helpful' votes to total votes that it received. The BERT classifier had an accuracy of 78.3\% while an RNN classifier, used as a comparison, had an accuracy of 73.9\%.

Research has been done into the prediction of the helpfulness of reviews on Steam; however, it does not seem that BERT has been used for this task. In \cite{eberhard2018investigating}, the helpfulness of a set of 130 thousand Steam reviews was predicted. The classifiers input features consisted of, among many other things, the time the reviewer spent playing the game, the reading level of the review text and the frequency with which swear words were used. The complete review text was not considered as an input feature itself. The number of `helpful' votes a review received determined its classification label: the bottom 80\% of reviews were labelled `unhelpful', the top 5\% of reviews were labelled `top' and the remaining 15\% of reviews were labelled `helpful'. A random forest classifier was used to classify the reviews with the resulting f1-scores, which ranged between 0.58 and 0.64, indicating mixed results.

In \cite{baowaly2019predicting}, a dataset consisting of over 2.6 million Steam reviews was used to train a gradient boosting machine model to predict the reviews' helpfulness. As in \cite{eberhard2018investigating}, many input features were considered, including the time the reviewer spent playing the game, the polarity of the review, certain manually extracted text features and, notably, the TF-IDF representation of the review text. A review's helpfulness was determined by the ratio of `helpful' votes to total votes that it received. The trained model was used as both a classifier and a regressor with multiple helpfulness ratio thresholds being considered when making predictions. As a classifier, the model produced f1-scores that ranged between 0.74 and 0.94 depending on the chosen threshold. As a regressor, the model had a root mean square error of 0.17.

\section{Representative Users} \label{sec:LR_Rep}

The study, identification and prediction of influential or representative users in social networks often focuses on the social graph of users as the primary, or only, source of information \cite{trusov2010determining} \cite{ghosh2010predicting}.

In \cite{choi2013representative}, individual product reviewers who accurately represent the opinions or ratings of many users are compared to influential users that can be found in social networks. An algorithmic method to identify representative users was developed and applied to two datasets consisting of music and movie reviews, respectively. This algorithm produced positive results when applied to the datasets in question with the authors concluding that the results indicated that ``well-representative users can be identified among raters in internet social media''. Influential product reviews have also been studied for the purpose of viral marketing research, such as in \cite{li2009discovering}, where product reviews from Epinions.com were used to train a neural network to predict a reviewer's product marketing influence based on the number of `ratings' their reviews received. The relationship between the helpfulness of reviews, discussed in section \ref{sec:LR_NLP_Help}, and the purchasing behaviour of customers was investigated in \cite{malik2020exploring}, as were the features of a reviewer that led to them being considered helpful.

In \cite{kamath2016understanding}, the ratings of restaurants on Yelp were predicted, using an SVM regressor, by extracting the topics mentioned in users' reviews of those restaurants, determining their overall sentiment towards those topics across all of their reviews and evaluating a matrix of all of the resulting topic sentiments of each user who had reviewed a given restaurant. This approach, while utilising the review text to extract topics, did not use the raw text of the reviews as input features themselves, nor did it attempt to determine which reviewers were particularly representative of the overall population.

\section{Steam} \label{sec:LR_Steam}

As discussed in section \ref{sec:LR_NLP}, a substantial amount of research has been done into the classification and prediction of both the polarity and helpfulness of reviews on Steam; however, models such as BERT have not yet been applied to such tasks. Multiple collections of Steam data have also been published openly and used for research such as the one in \cite{zuo2018sentiment}, which has already been discussed, and another gathered in \cite{o2016condensing}. Most of the remaining research into Steam involving the aforementioned datasets appears to concern the development of game recommendation systems for users of the platform such as in \cite{saaidin2020recommender}, \cite{wang2020using} and \cite{linsteam}. There does not appear to be any existing research into the identification or prediction of particularly influential or representative reviewers on Steam, in particular.
