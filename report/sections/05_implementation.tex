\chapter{Design and Implementation} \label{sec:DI}

This chapter will discuss, in detail, the methods used and the approaches taken when implementing solutions to the two primary tasks this dissertation is concerned with: the prediction of various review features using only the review text and the identification of users who best represent the population of reviewers, as a whole, through the content of their written reviews.

\section{Review Feature Prediction} \label{sec:DI_RF}

In this section, the process of selecting and training optimal machine learning models with the goal of predicting three features of a given review will be discussed. The review features being predicted are its polarity, how many helpfulness votes it received and the length of time the reviewer spent playing the game. The machine learning models that will be tested are BERT, specifically two pretrained DistilBERT models; MNB and CNB classifiers; linear SVMs for classification and regression, both with SGD and without; a ridge regressor; and multiple baseline classifiers and regressors.

\subsection{Dataset Sampling} \label{sec:DI_RF_Sampling}

Due to the time and memory-consuming nature of the model training procedure in machine learning, especially when considering neural networks like BERT, it would be unfeasible to utilise the entirety of the dataset, which consists of over 10 million reviews, in the model training and testing processes. Instead, numerous samples of the dataset, each containing exactly 100 thousand reviews, were prepared and used to train the predictive models.

Only reviews consisting of at least one word, determined by the process outlined in section \ref{sec:Dataset_RT_Langs}, were included when preparing the dataset samples. The language that a review was written in was considered a determining factor in relation to how well the trained models would be able to accurately predict the selected review features. As such, separate samples were prepared for reviews written in English\footnote{Reviews that were classified as English with a confidence of at least 70\% by the language detection algorithm discussed in \ref{sec:Dataset_RT_Langs} were included in the English-language samples.} and reviews written in any language (including English). Also of interest was the potential effect that the length of the review text, ie the word count, might have on the accuracy of the trained models' predictions. As such, separate samples were prepared for reviews that contained 50 words or more, referred to as `long' reviews; reviews that contained fewer than 50 words, referred to as `short' reviews; and reviews containing any number of words.

Several samples were also prepared solely for the task of review polarity prediction. Due to the imbalanced nature of the dataset with regard to review polarity, 85\% of reviews are positive while only 15\% of reviews are negative, and, since this task is a classification problem, separate samples were prepared, containing equal numbers of positive and negative reviews in order to more accurately determine how well the trained models were performing.

The names and characteristics of all of the prepared data samples are given in Table \ref{tab:DI_RF_Datasets}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|l l l l}
        \toprule
        \textbf{Sample Name} & \textbf{Language} & \textbf{Length} & \textbf{Balanced} & \textbf{Size}\\\midrule
        \texttt{eng\_eq\_any} & English & Any & Yes & 100k\\
        \texttt{eng\_eq\_short} & English & Short & Yes & 100k\\
        \texttt{eng\_eq\_long} & English & Long & Yes & 100k\\
        \texttt{any\_eq\_any} & Multilingual & Any & Yes & 100k\\
        \texttt{any\_eq\_short} & Multilingual & Short & Yes & 100k\\
        \texttt{any\_eq\_long} & Multilingual & Long & Yes & 100k\\
        \texttt{eng\_any\_any} & English & Any & No & 100k\\
        \texttt{eng\_any\_short} & English & Short & No & 100k\\
        \texttt{eng\_any\_long} & English & Long & No & 100k\\
        \texttt{any\_any\_any} & Multilingual & Any & No & 100k\\
        \texttt{any\_any\_short} & Multilingual & Short & No & 100k\\
        \texttt{any\_any\_long} & Multilingual & Long & No & 100k\\
        \bottomrule
    \end{tabular}
    \caption{Dataset samples and their characteristics for review feature prediction.}
    \label{tab:DI_RF_Datasets}
\end{table}

\subsection{Polarity} \label{sec:DI_RF_Pol}

\subsubsection{BERT}

\paragraph{Data Splitting}

Each dataset sample was split into three sets: training (80\% of the sample), validation (10\% of the sample) and test (10\% of the sample). The training set was used to fine-tune the BERT model, the validation set was used to evaluate the trained model after each epoch and the test set was used to determine the overall quality of the fully trained model.

\paragraph{Preprocessing}

Very little preprocessing needed to be done to the data before it was passed into the BERT model for training. Tokenisation was done using the pretrained tokenisers that were provided with both the English-language and multilingual DistilBERT models. The tokenised input data, alongside the output polarity data, were converted to tensors, before being used to fit the BERT classifier.

\paragraph{Hyperparameter Tuning}

Optimal training parameters were selected for all of the BERT models used to predict review polarity by testing numerous combinations of parameters on a model trained using the \texttt{eng\_eq\_any} sample. Individually applying the hyperparameter tuning process to each of the dataset samples, even if it may have led to slightly better results for those samples, would have been far too time-consuming to carry out.

The learning rate of the Adam optimiser, which is used by the classifier, was originally considered as a tunable parameter; however, experimenting with its value did not lead to any improvements and, as such, it was not considered in the final approach. The two parameters which were settled on were the batch size and the number of epochs the model was to be trained for. The parameter values that were considered can be seen in Table \ref{tab:DI_RF_Pol_BERTHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        Batch size & $16, 32, 64$\\
        Epochs & $2, 3, 4$\\
        \bottomrule\\
    \end{tabular}
    \caption{BERT hyperparameters for review polarity.}
    \label{tab:DI_RF_Pol_BERTHP}
\end{table}

The training and validation accuracies of the trained models for each combination of hyperparameters can be seen in Figure \ref{fig:DI_RF_Pol_BERTHP}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/05_impl/01_rfp/01_pol/plot_hyperparams_bert.png}
    \caption{BERT hyperparameter tuning results for review polarity ($n$ is batch size).}
    \label{fig:DI_RF_Pol_BERTHP}
\end{figure}

Based on these results it is clear that the classifiers begin to overfit the training data after 2 epochs as the training accuracy begins to increase at the expense of the validation accuracy. The models trained with a batch size of 64 also appear to perform the strongest and, as such, the combination of 2 epochs and a batch size of 64 will be used when training and testing BERT models for the other dataset samples.

\paragraph{Training Procedure}

BERT models were trained and validated for all 12 dataset samples using the optimal hyperparameters. Samples containing English reviews were trained using the pretrained \texttt{distilbert-base-uncased} model while multilingual samples were trained using the \texttt{distilbert-base-multilingual-cased} model. Their performance was then tested using the test sets of their respective samples. The results of these tests will be discussed, in detail, in section \ref{sec:Res_RF_Pol}.

\subsubsection{Other Models}

\paragraph{Data Splitting}

In contrast to the procedure utilised when training BERT models, the dataset samples were split into two sets rather than three: training (90\% of the sample) and test (10\% of the sample). The training results were gathered using the mean accuracy of 5-fold cross-validation performed on the training set as opposed to the validation accuracy taken from the BERT training process. The test results were gathered in the same manner as the BERT test results were gathered.

\paragraph{Preprocessing}

Unlike with BERT, significant preprocessing needed to be applied to the review text data before it could be passed into a model for training. The steps involved in this process are as follows:

\begin{enumerate}
    \item Convert the text to lowercase\footnote{Only for English-language reviews.}.
    \item Replace all non-word characters\footnote{Any Unicode character that is not a letter used in words in any language.} with spaces.
    \item Split the text into words based on spaces.
    \item Remove any stopwords.
    \item Join the remaining words back together and remove repeated spaces.
    \item Vectorise the text.
    \item Apply TF-IDF weightings to the vectorised text.
\end{enumerate}

The text was also processed into various sets of $n$-grams before being used in model training; however, this process will be discussed in the following section. The list of stopwords used was provided by Python's NLTK library \cite{bird2009natural} which contains stopword lists for 24 different languages.

\paragraph{Hyperparameter Tuning}

As was the case with BERT, optimal hyperparameters for each model were selected based on the results gathered for the \texttt{eng\_eq\_any} sample due to the time-consuming nature of the training process.

Regardless of the model being used, the review text was converted into sets of $n$-grams of varying sizes and ranges\footnote{The $n$-gram range of $(1, 2)$, for example, would consist of all unigrams and bigrams taken from the text.}. For the MNB, CNB and SGD classifiers, the $\alpha$ parameter was varied. For the LSVC, the $C$ parameter was varied. Parameter ranges were chosen so as to provide a large amount of variance between results so that the optimal parameters for each model could be accurately selected. The parameter values that were considered can be seen in Table \ref{tab:DI_RF_Pol_BaseHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Model} & \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        All & $n$-gram range & $(1, 2), (1, 3), (1, 4)$\\
        MNB & $\alpha$ & $0.01, 0.1, 1, 10$\\
        CNB & $\alpha$ & $0.01, 0.1, 1, 10$\\
        SGD & $\alpha$ & $10^{-4}, 10^{-5}, 10^{-6}$\\
        LSVC & $C$ & $0.01, 0.1, 1, 10$\\
        \bottomrule\\
    \end{tabular}
    \caption{Other model hyperparameters for review polarity.}
    \label{tab:DI_RF_Pol_BaseHP}
\end{table}

Two baseline classifiers were also used for comparison: one which predicted a random class and one which predicted the most frequent class. The mean cross-validation accuracies for each combination and model can be seen in Figure \ref{fig:DI_RF_Pol_BaseHP}. The standard deviations of the cross-validation accuracies were also gathered; however, they were all found to be insignificantly small and, as such, have not been included in the plot.

\begin{figure}[ht]
    \hspace*{-0.3in}
    \includegraphics[scale=0.55]{figures/05_impl/01_rfp/01_pol/plot_hyperparams_base.png}
    \caption{Other model hyperparameter tuning results for review polarity.}
    \label{fig:DI_RF_Pol_BaseHP}
\end{figure}

Based on these results it seems that, for each SGD classifier, the parameter value of $\alpha=10^{-5}$ is optimal. The $n$-gram range of $(1, 3)$ also seems to have the best performance for that $\alpha$, though not by much. For the LSVC, a parameter value of $C=1$ with an $n$-gram range of $(1, 3)$ appears to be the best choice, although $C=0.1$ with an $n$-gram range of $(1, 2)$ performs almost as well.

Both of the NB classifiers seem to produce very similar results for each choice of $\alpha$ and $n$-gram range. In each case, it is clear that a parameter value of $\alpha=1$ with an $n$-gram range of $(1,4)$ is the best performing combination.

\paragraph{Training Procedure}

Each of the optimal classifiers was trained on all 12 dataset samples and their performance was then tested using the test sets of their respective samples. The test results will be discussed, and compared to the BERT results, in section \ref{sec:Res_RF_Pol}.

\subsection{Votes} \label{sec:DI_RF_Votes}

The process of training the machine learning models to predict the number of helpfulness votes that reviews received was very similar to the process used to predict their polarity. One key difference in the procedure was that the balanced dataset samples, ie those with 50\% positive and 50\% negative reviews, were not relevant to the task and were, as a result, not included.

\subsubsection{BERT}

\paragraph{Data Splitting}

The dataset samples were split into the same three sets, with the same proportions, that were mentioned for the BERT model in section \ref{sec:DI_RF_Pol}.

\paragraph{Preprocessing}

As was also discussed previously, very little preprocessing needed to be done to the text data before being passed into the BERT model. However, a robust scaler was used to map the number of votes each review received down to a limited range centred around 0. A robust scaler was used due to the presence of outliers in the dataset.

\paragraph{Hyperparameter Tuning}

Optimal training parameters were selected for all of the BERT models based on the results gathered for the \texttt{eng\_any\_any} sample. The same combinations of batch size and training epochs were tested for the prediction of review votes as were tested for the prediction of review polarity. These values can be seen, again, in Table \ref{tab:DI_RF_Votes_BERTHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        Batch size & $16, 32, 64$\\
        Epochs & $2, 3, 4$\\
        \bottomrule\\
    \end{tabular}
    \caption{BERT hyperparameters for review votes.}
    \label{tab:DI_RF_Votes_BERTHP}
\end{table}

The NMSE loss for both the training and validation sets can be seen in Figure \ref{fig:DI_RF_Votes_BERTHP}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/05_impl/01_rfp/02_votes/plot_hyperparams_bert.png}
    \caption{BERT hyperparameter tuning results for review votes ($n$ is batch size).}
    \label{fig:DI_RF_Votes_BERTHP}
\end{figure}

Interestingly, based on these results, it seems that the trained models might be underfitting the training data due to the superior performance of the model against the validation set. Regardless, the best results were produced when a batch size of 64 was used. For a batch size of 64, the results for both 2 and 3 training epochs are almost identical and so, in the interest of reducing training time, 2 epochs would appear to be the optimal choice.

\paragraph{Training Procedure}

The same training procedure was used for the prediction of reviews votes as was used for the prediction of review polarity with the only significant difference being the number of dataset samples being considered.

\subsubsection{Other Models}

\paragraph{Data Splitting}

The dataset splitting procedure was identical to the one outlined for the other models in section \ref{sec:DI_RF_Pol}.

\paragraph{Preprocessing}

The numerous preprocessing steps that were applied to the review text when predicting polarity were also applied here. An additional preprocessing step, discussed already in this section, was the use of a robust scaler to map the number of votes down to a limited range while accounting for outliers.

\paragraph{Hyperparameter Tuning}

Optimal hyperparameters were selected for each model based on the results gathered for the \texttt{eng\_any\_any} sample. The review text was, again, converted into sets of $n$-grams of varying sizes and ranges. For the SGD and ridge regressors, the $\alpha$ parameter was varied. For the LSVR, the $C$ parameter was varied. The methodology used to select parameter ranges, to provide varied results to better identify optimal values, was again applied. The parameter values that were considered can be seen in Table \ref{tab:DI_RF_Votes_BaseHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Model} & \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        All & $n$-gram range & $(1, 2), (1, 3), (1, 4)$\\
        Ridge & $\alpha$ & $0.01, 0.1, 1, 10$\\
        SGD & $\alpha$ & $10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}$\\
        LSVR & $C$ & $0.01, 0.1, 1, 10$\\
        \bottomrule\\
    \end{tabular}
    \caption{Other model hyperparameters for review votes.}
    \label{tab:DI_RF_Votes_BaseHP}
\end{table}

Two baseline regressors were also used for comparison: one which predicted the mean number of scaled votes and one which predicted the median number of scaled votes. The mean cross-validation NMSE losses, along with the standard deviations, can be seen in Figure \ref{fig:DI_RF_Votes_BaseHP}. In order to improve the clarity of the plots in Figure \ref{fig:DI_RF_Votes_BaseHP}, some results have been offset slightly on the x-axis.

\begin{figure}[ht]
    \hspace*{-0.3in}
    \includegraphics[scale=0.55]{figures/05_impl/01_rfp/02_votes/plot_hyperparams_base.png}
    \caption{Other model hyperparameter tuning results for review votes.}
    \label{fig:DI_RF_Votes_BaseHP}
\end{figure}

The standard deviations of the NMSE losses for each model, despite being quite large, are almost identical and, as such, do not play an important role in determining the optimal choices of hyperparameters.

For the SGD regressor, it would appear that a parameter value of $\alpha=10^{-4}$ with an $n$-gram range of $(1, 2)$ is the best choice; although, no combination of parameters appeared to perform particularly well. For the ridge regressor, the optimal choice is slightly more obvious: a parameter value of $\alpha=10$ with an $n$-gram range of $(1, 2)$ produced the best results. For the LSVR, a parameter value of $\alpha=10$ also produces the best results. An $n$-gram range of $(1, 3)$ produced slightly better results than the other ranges, although this is not clear from the plots alone.

\paragraph{Training Procedure}

Each of the optimal regressors was trained and tested on the 6 imbalanced dataset samples. The test results will be discussed in section \ref{sec:Res_RF_Votes}.

\subsection{Playtime} \label{sec:DI_RF_PT}

The process of training the machine learning models to predict the amount of time that reviewers spent playing the games they reviewed was almost identical to the process used to predict the number of helpfulness votes reviews received.

The data splitting, preprocessing\footnote{A robust scaler was also applied to the playtime values due to a similar presence of outliers.} and training procedure steps involved in the prediction of review playtime, regardless of the model being trained, are identical to the steps given in section \ref{sec:DI_RF_Votes} concerning the prediction of review votes. As such, they have been excluded.

\subsubsection{BERT}

\paragraph{Hyperparameter Tuning}

Optimal training parameters were, again, selected based on the results gathered for the \texttt{eng\_any\_any} sample. Identical combinations of batch size and training epochs were tested and can be seen, once again, in Table \ref{tab:DI_RF_PT_BERTHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        Batch size & $16, 32, 64$\\
        Epochs & $2, 3, 4$\\
        \bottomrule\\
    \end{tabular}
    \caption{BERT hyperparameters for review playtime.}
    \label{tab:DI_RF_PT_BERTHP}
\end{table}

The NMSE loss for both the training and validation sets can be seen in Figure \ref{fig:DI_RF_PT_BERTHP}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/05_impl/01_rfp/03_pt/plot_hyperparams_bert.png}
    \caption{BERT hyperparameter tuning results for review playtime ($n$ is batch size).}
    \label{fig:DI_RF_PT_BERTHP}
\end{figure}

It is quite clear from these results that the optimal choice hyperparameters are batch sizes of 32 or 64 in combination with 2 training epochs, with a batch size of 32 having a slightly better performance\footnote{Due to an initial misreading of the results, a batch size of 64 was chosen when training and testing BERT on the other dataset samples; however, since the NMSE losses for the batch size choices of 32 and 64 were almost identical, this misreading is unlikely to have a significant effect on the overall results.}.

\subsubsection{Other Models}

\paragraph{Hyperparameter Tuning}

The models, hyperparameters and parameter values that were used in the prediction of review votes were also used in the prediction of review playtime. These values can be seen in Table \ref{tab:DI_RF_PT_BaseHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Model} & \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        All & $n$-gram range & $(1, 2), (1, 3), (1, 4)$\\
        Ridge & $\alpha$ & $0.01, 0.1, 1, 10$\\
        SGD & $\alpha$ & $10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}$\\
        LSVR & $C$ & $0.01, 0.1, 1, 10$\\
        \bottomrule\\
    \end{tabular}
    \caption{Other model hyperparameters for review playtime.}
    \label{tab:DI_RF_PT_BaseHP}
\end{table}

The same two baseline regressors, one that predicted the mean playtime and one that predicted the median playtime, were used for comparison.

The mean cross-validation NMSE losses, along with the standard deviations, can be seen in Figure \ref{fig:DI_RF_PT_BaseHP}. Certain results were, again, offset slightly on the x-axis in order to improve the clarity of the plots.

\begin{figure}[ht]
    \hspace*{-0.3in}
    \includegraphics[scale=0.55]{figures/05_impl/01_rfp/03_pt/plot_hyperparams_base.png}
    \caption{Other model hyperparameter tuning results for review playtime.}
    \label{fig:DI_RF_PT_BaseHP}
\end{figure}

The standard deviations of the NMSE losses for each model do not play an important role in the selection of hyperparameters for the same reasons listed in \ref{sec:DI_RF_Votes}.

For the SGD regressor, the $n$-gram range of $(1, 2)$ clearly produces the best results, with the parameter value of $\alpha=10^{-5}$ performing slightly better than the other options. For the ridge regressor, the optimal choices of an $n$-gram range of $(1, 2)$ and a parameter value of $\alpha=10$ are quite clear. For the LSVR, a value of $C=10$ is clearly the optimal choice. Although it isn't clear from the plot, an $n$-gram range of $(1, 3)$ produces the best results when $C=10$.

\section{Representative Users} \label{sec:DI_RU}

In this section, the process of selecting and training optimal machine learning models with the goal of identifying users whose written reviews best represent the views of the total population will be discussed. The machine learning models that will be tested are the previously discussed DistilBERT models; MNB and CNB classifiers; a linear SVM classifier, both with SGD and without; and multiple baseline classifiers.

\subsection{Methodology}

The methodology underpinning this approach can be described as follows:

\begin{enumerate}
    \item Select the reviews of users who've written at least $N$ reviews.
    \item Calculate the average rating, across the entire dataset, for each of the games being reviewed.
    \item Convert these ratings into classes that correspond to Steam's own rating classification scheme.
    \item Train models to predict the rating class of a game being reviewed using only the review text.
    \item Test this model on a per-user basis and determine the users whose reviews are predicted most accurately.
    \item These users could be considered `representative' of the reviewer population.
\end{enumerate}

Much of the steps involved in this approach are essentially extensions to the work done to predict a review's polarity in section \ref{sec:DI_RF_Pol}. The goal, in this case, is to train a multiclass classifier to predict the average polarity of a game based on all of its reviews, rather than to predict the binary class of an individual review.

A potential drawback to this approach is that, given a game with a positive rating of 70\%, we would expect 30\% of the review texts for that game, ie the negative ones, to portray a noticeably negative sentiment. This could be problematic since, in this example, 30\% of the training data could be considered bad or of poor quality because the overall rating ought to be fairly positive. A key assumption is made in an attempt to overcome this drawback. This assumption is as follows: since a significant majority of the review data being processed for each game does actually align with its overall rating, the predictive model ought to be able to, on average, favour the good data and ignore the bad, ie noisy, data since there always exists much more good data.

Another assumption made by this approach is that there are features within the review texts which occur more frequently for games of different average ratings. For example, we might expect extremely positive words like `amazing' or `fantastic' to occur more in reviews for games with a positive rating of 95\% than in reviews for games with a positive rating of 70\%. The hope is that the trained model will be able to discern these particular features from the otherwise noisy data.

\subsection{Dataset Sampling and Preparation}

Numerous samples of the dataset, each containing exactly 160 thousand reviews, were prepared and used to train the predictive models.

First, reviews consisting of less than one word and reviews for `early access' games were excluded from the dataset. Reviews for games with fewer than 50 total reviews were also excluded. Finally, out of those reviews remaining, the reviews of users who had written fewer than 10 written reviews were excluded.

For those reviews that remained, the average rating for each game was calculated and mapped to one of either 3 or 6 classes. These classes were chosen and named based roughly on the chart shown in Figure \ref{fig:SteamStore_Ratings}. The details of each of these sets of classes are given in Table \ref{tab:DI_PU_Classes6} and Table \ref{tab:DI_PU_Classes3}, respectively.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Name} & \textbf{Acronym} & \textbf{Rating Range}\\\midrule
        Very Positive & VP & $[0.95, 1.0]$\\
        Positive & P & $[0.8, 0.95)$\\
        Mostly Positive & MP & $[0.7, 0.8)$\\
        Mixed & M & $[0.4, 0.7)$\\
        Mostly Negative & MN & $[0.2, 0.4)$\\
        Negative & N & $[0.0, 0.2)$\\
        \bottomrule\\
    \end{tabular}
    \caption{Game rating classes used in 6-class approach.}
    \label{tab:DI_PU_Classes6}
\end{table}

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Name} & \textbf{Acronym} & \textbf{Rating Range}\\\midrule
        Positive & P & $[0.7, 1.0]$\\
        Mixed & M & $[0.4, 0.7)$\\
        Negative & N & $[0.0, 0.4)$\\
        \bottomrule\\
    \end{tabular}
    \caption{Game rating classes used in 3-class approach.}
    \label{tab:DI_PU_Classes3}
\end{table}

Since the goal of this task is to determine representative users, it is important that, for any particular user with reviews in a dataset sample, all of the valid reviews they've written are included and none of these reviews are split across different sample splits, eg the training and test splits. Because of this requirement, the training, validation and test splits were prepared in advance, rather than at runtime. The sample splitting was carried out so that the reviews of 80\% of users were in the training set, the reviews of 10\% of users were in the validation set and the reviews of the remaining 10\% of users were in the test set.

Separate samples were prepared for reviews written in English and reviews written in any language (including English). The names and characteristics of all of the prepared data samples are given in Table \ref{tab:DI_PU_Samples}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|l l l}
        \toprule
        \textbf{Sample Name} & \textbf{Language} & \textbf{Classes} & \textbf{Size}\\\midrule
        \texttt{eng\_6} & English & 6 & 160k\\
        \texttt{eng\_3} & English & 3 & 160k\\
        \texttt{any\_6} & Multilingual & 6 & 160k\\
        \texttt{any\_3} & Multilingual & 3 & 160k\\
        \bottomrule
    \end{tabular}
    \caption{Dataset samples and their characteristics for determining representative users.}
    \label{tab:DI_PU_Samples}
\end{table}

\subsection{BERT}

\subsubsection{Preprocessing}

As was the case when predicting the polarity of reviews, very little preprocessing was needed before passing the data into the BERT model for training. The data, which had already been split into training, validation and test sets, were converted to tensors and used to fit the BERT classifier.

\subsubsection{Hyperparameter Tuning}

Optimal training parameters were selected for all of the models using the results gathered by training models on the \texttt{eng\_6} sample. The same BERT hyperparameters that were varied when predicting the review features were also varied in this approach. The considered values can be seen in Table \ref{tab:DI_PU_BERTHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l}
        \toprule
        \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        Batch size & $16, 32, 64$\\
        Epochs & $2, 3, 4$\\
        \bottomrule\\
    \end{tabular}
    \caption{BERT hyperparameters for representative users.}
    \label{tab:DI_PU_BERTHP}
\end{table}

The training and validation accuracies of the trained models for each combination of hyperparameters can be seen in Figure \ref{fig:DI_PU_BERTHP}.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.7]{figures/05_impl/02_pu/plot_hyperparams_bert.png}
    \caption{BERT hyperparameter tuning results for representative users ($n$ is batch size).}
    \label{fig:DI_PU_BERTHP}
\end{figure}

Based on these results it would appear that 2 training epochs with a batch size of 64 are the optimal combination of parameters. Interestingly, the validation and training accuracies for this combination are nearly identical, indicating that the model is neither underfitting nor overfitting the training data.

\subsubsection{Training and Testing Procedure}

BERT models were trained and validated for all 4 dataset samples using the aforementioned combination of hyperparameters and the appropriate choice of pretrained DistilBERT model, which depended on the language of the samples' reviews. Their performance was then tested using the relevant test set. The overall results of these tests, as well as the user-specific results, will be discussed in section \ref{sec:Res_RU}.

\subsection{Other Models}

\subsubsection{Preprocessing}

The same preprocessing steps taken when predicting review polarities for non-BERT models in section \ref{sec:DI_RF_Pol} were also taken here. One important difference was that the previously prepared training and validation splits were merged into a single training set containing the reviews of 90\% of users.

\subsubsection{Hyperparameter Tuning}

Optimal model parameters were again selected based on the results taken from the \texttt{eng\_6} sample.

The models, hyperparameters and values that were varied were identical to those used in section \ref{sec:DI_RF_Pol}. These values can be seen in Table \ref{tab:DI_PU_BaseHP}.

\begin{table}[ht]
    \centering
    \begin{tabular}{l l l}
        \toprule
        \textbf{Model} & \textbf{Hyperparameter} & \textbf{Values}\\\midrule
        All & $n$-gram range & $(1, 2), (1, 3), (1, 4)$\\
        MNB & $\alpha$ & $0.01, 0.1, 1, 10$\\
        CNB & $\alpha$ & $0.01, 0.1, 1, 10$\\
        SGD & $\alpha$ & $10^{-4}, 10^{-5}, 10^{-6}$\\
        LSVC & $C$ & $0.01, 0.1, 1, 10$\\
        \bottomrule\\
    \end{tabular}
    \caption{Other model hyperparameters for representative users.}
    \label{tab:DI_PU_BaseHP}
\end{table}

Two baseline classifiers were also used for comparison: one which predicted the most frequent class and one which predicted a random class based on the overall distribution of the classes in the dataset. The mean cross-validation accuracies can be seen in Figure \ref{fig:DI_PU_BaseHP}. The standard deviations, as was the case when predicting review polarities, were found to be insignificantly small and, as such, have not been included.

\begin{figure}[ht]
    \hspace*{-0.3in}
    \includegraphics[scale=0.55]{figures/05_impl/02_pu/plot_hyperparams_base.png}
    \caption{Other model hyperparameter tuning results for representative users.}
    \label{fig:DI_PU_BaseHP}
\end{figure}

For the SGD classifier, it is quite clear that the optimal choice of hyperparameters is an $n$-gram range of $(1, 2)$ and a parameter value of $\alpha=10^{-5}$. For both NB classifiers, an $n$-gram range of $(1, 2)$ produced the best results. A parameter value of $\alpha=0.1$ produced the best results for the MNB classifier, while a value of $\alpha=1$ produced the best results for the CNB classifier. Finally, for the LSVC, the combination of hyperparameters that performed the best was an $n$-gram range of $(1, 2)$ with a parameter value of $C=1$. Interestingly, regardless of the classifier in question, an $n$-gram range $(1, 2)$ always produced better results than the other ranges.

\subsubsection{Training and Testing Procedure}

Models were trained and tested for all of the dataset samples using the optimal choices of hyperparameters. As was the case with the BERT model, the performance of the models was evaluated with regard to the entire test set as well as on a per-user basis. These results will be discussed in section \ref{sec:Res_RU}.
